{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement:**\n",
    "\n",
    "Avocado is a fruit consumed by people heavily in the United States. \n",
    "\n",
    "**Content**\n",
    "This data was downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV. \n",
    "\n",
    "The table below represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. \n",
    "\n",
    "Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. \n",
    "\n",
    "The Product Lookup codes (PLU’s) ('4046','4225','4770') in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.\n",
    "\n",
    "Some relevant columns in the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![]('data_image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data_image.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import zscore \n",
    "from sklearn.decomposition import PCA\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split ,  cross_val_score\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.ensemble import  GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import  Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv('avocado.csv')\n",
    "adf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There  are lots of Null Columns , So we will delete these rows having no values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = adf.dropna(how = 'any' , axis = 0)\n",
    "adf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming some columns for our further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = adf.rename(columns = {'4046' : 'Small_Medium' , '4225' : 'Large' , '4770' : 'Extra_Large' })\n",
    "adf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['Unnamed: 0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the dataset to obtain - Count , Mean , Standard deviation , Mininmum , IQR , Maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Unnamed column as it is some some random indexing provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = adf.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adf.shape)\n",
    "print(adf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Information about the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that date,type and region are of object type so we need to treat them ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date**\n",
    "\n",
    "Here the date provided is in object format so we will convert its datatype to Datetime , and split Year , Month and Day from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['Date']=pd.to_datetime(adf['Date'])\n",
    "adf['Sold_Year'] = adf['Date'].apply(lambda x:x.year)\n",
    "adf['Sold_Month']=adf['Date'].apply(lambda x:x.month)\n",
    "adf['Sold_Day']=adf['Date'].apply(lambda x:x.day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting graph to visualise how **Average Price** varies accordingly with **Date**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dategroup=adf.groupby('Date').mean()\n",
    "plt.figure(figsize=(12,5))\n",
    "dategroup['AveragePrice'].plot(x=adf.Date , kind = 'line' , colormap = 'PuBuGn_r')\n",
    "plt.title('Average Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the **Average price**  -  **region** wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot('AveragePrice','region',data= adf,\n",
    "                   hue='year',\n",
    "                   height= 10,\n",
    "                   aspect=0.9,\n",
    "                   palette='GnBu_r',\n",
    "                   join=False,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the Date column as we have extracted Year , Month , Date from it . Hence we will not use this Date column for our further predictions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = adf.drop('Date', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the type column all the avocados are conventional so we can drop this column as all the datas are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = adf.drop('type' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Region**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "region column has the names of various regions , so we need to encode this column ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Label Encoder except of Onehot Encoder as One Hot encoder will create different columns for different regions so we'll use Label Encoder ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "adf['region'] = encoder.fit_transform(adf['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_copy = adf[\"region\"]\n",
    "reg_dup = adf_copy.drop_duplicates()\n",
    "reg_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the above code there are two columns -\n",
    "First one shows the indexes of the region name \n",
    "and\n",
    "second one shows the entity in which particular index is been converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Number of zeros in a column in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from above we can see that columns Extra_Large , Large Bags , XLarge Bags has some zero values so we need to deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_in_XL = adf['XLarge Bags'].astype(bool).sum(axis = 0)\n",
    "\n",
    "percentage_of_zeroes = (z_in_XL/1517)*100\n",
    "percentage_of_zeroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can oobserve that XLarge has 47% zeroes , so we will drop it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = adf.drop('XLarge Bags' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting boxplot to visualise Outliers in Average Price in a respective Year ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "sns.boxplot(x='Sold_Year',y='AveragePrice',data=adf,color='darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LMPLOT*\n",
    "\n",
    "This function combines regplot() and FacetGrid. It is intended as a convenient interface to fit regression models across conditional subsets of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Lmplot to visualise affect on target variable ('Average Price') with predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in adf:\n",
    "    sns.lmplot(x = i , y ='AveragePrice' , data = adf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Boxplot.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Boxplot for visualising Outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist=adf.columns.values\n",
    "ncol=4\n",
    "nrows=4\n",
    "plt.figure(figsize=(16, 9 ))\n",
    "for i in range (0,len(collist)):\n",
    "    plt.subplot(nrows,ncol,i+1)\n",
    "    sns.boxplot(adf[collist[i]],color='darkcyan',orient='h')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding zscore of our dataframe ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(adf))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering values from the dataframe whose zscore<3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_wo = adf[(z<3).all(axis = 1)]\n",
    "adf_wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our dataframe size has been reduced to 1471 from 1517."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_wo.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we have dropped 3 columns Date , type , XLarge Bags . And now also we can observe that there are two year columns in our dataset . So we need to drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_wo = adf_wo.drop('year' , axis = 1)\n",
    "adf_wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predictor = adf_wo.drop('AveragePrice', axis = 1)\n",
    "x_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predictor.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting distplot to visualize skewness in every colummns .\n",
    "\n",
    "*Distplot*\n",
    "\n",
    "This function combines the matplotlib hist function (with automatic calculation of a good default bin size) with the seaborn kdeplot() and rugplot() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in x_predictor :\n",
    "    sns.distplot(x_predictor[feature] , kde = True , color = 'darkcyan' )\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "powert = PowerTransformer( method = 'yeo-johnson' , standardize = False)\n",
    "x_t = powert.fit_transform(x_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans = pd.DataFrame(x_t , columns = x_predictor.columns)\n",
    "x_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in x_trans :\n",
    "    sns.distplot(x_trans[feature] , kde = True , color = 'darkcyan' )\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trans.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule of thumb seems to be: If the skewness is between -0.5 and 0.5, the data are fairly symmetrical. If the skewness is between -1 and – 0.5 or between 0.5 and 1, the data are moderately skewed. If the skewness is less than -1 or greater than 1, the data are highly skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for min and max values for each column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_trans :\n",
    "    print(i , max(x_trans[i]) - min(x_trans[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian's distribution with zero mean and unit variance is standard scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_s = scaler.fit_transform(x_trans)\n",
    "x_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sc = pd.DataFrame(x_s , columns = x_trans.columns)\n",
    "x_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From above we can observe that some of the columns are scaled to zero . So we'll not scale our data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "sns.heatmap(x_sc.corr() , annot = True , fmt = '.2%' , cmap = 'YlGnBu_r') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see there is so much correlation between different columns so we will use PCA(principal component analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 'mle' , svd_solver = 'full' )\n",
    "xpca = pca.fit_transform(x_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_f = pd.DataFrame(xpca )\n",
    "x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adf_wo.iloc[: , 0]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, train_size = 0.7 , test_size = 0.3, random_state = i)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_predicted = lr.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "    \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f , y, train_size=0.7, test_size=0.3, random_state= 1117)\n",
    "\n",
    "model_reg = [LinearRegression,RandomForestRegressor, SVR, DecisionTreeRegressor,KNeighborsRegressor, GradientBoostingRegressor,\n",
    "             ExtraTreesRegressor ,AdaBoostRegressor , Lasso , Ridge , ElasticNet ]\n",
    "\n",
    "\n",
    "for model in model_reg:\n",
    "    m = model()\n",
    "    print('\\n''Model: ',m)\n",
    "    m.fit(X_train, y_train)\n",
    "    scr=m.score(X_train,y_train)\n",
    "    score = (m.score(X_test , y_test))\n",
    "    print('\\n''-->''Score:',score)\n",
    "    scr_cross=cross_val_score(m,x_f,y,cv=5)\n",
    "    scr_mean=scr_cross.mean()\n",
    "    print('Cross validation score: ',scr_mean)\n",
    "    print('Difference between accuracy and cross validation score: ', scr-scr_mean)\n",
    "    y_predicted = m.predict(X_test)\n",
    "    print('Mean Absolute Error: ',mean_absolute_error(y_test, y_predicted))\n",
    "    print('R2 Score' , r2_score(y_test , y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use \n",
    "\n",
    "**Random Forest Regressor** ,\n",
    "\n",
    "**SVR** ,\n",
    "\n",
    "**Decision Tree Regressor** , \n",
    "\n",
    "**K Nearest Neighbors Regressor** ,\n",
    "\n",
    "**Gradient Boost Regressor** ,\n",
    "\n",
    "**Extra Trees Regressor** ,\n",
    "\n",
    "**AdaBoost Regressor**\n",
    "\n",
    "for our future predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, train_size = 0.7 , test_size = 0.3, random_state = i)\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_predicted = rf.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "    \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.3, random_state = 40)\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train,y_train)\n",
    "rf_pred=rf.predict(X_test)\n",
    "print('The R2 score={}'.format(r2_score(y_test, rf_pred)*100))\n",
    "print('The MSE ={}'.format(mean_squared_error(y_test, rf_pred)))\n",
    "print('The MAE ={}'.format(mean_absolute_error(y_test, rf_pred)))\n",
    "print('The RMSE ={}'.format(np.sqrt(mean_squared_error(y_test, rf_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Regressor Hyperparameter Tuning\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "btss = BlockingTimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rfr_params = {\"n_estimators\":[100,200],\n",
    "              \"criterion\":['mse'],\n",
    "              \"max_depth\":[6,8,10,20],\n",
    "             }\n",
    "gsRFC = GridSearchCV(rf,param_grid = rfr_params, cv=btss, scoring=\"r2\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train,y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "gsRFC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y_test, rf_pred , scatter = True , label = True , color = 'darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = i)\n",
    "    sv = SVR()\n",
    "    sv.fit(X_train,y_train)\n",
    "    y_predicted = sv.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "        \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = 40)\n",
    "\n",
    "sv = SVR()\n",
    "sv.fit(X_train , y_train)\n",
    "sv_pred = sv.predict(X_test)\n",
    "print('The R2 score={}'.format(r2_score(y_test, sv_pred)*100))\n",
    "print('The MSE ={}'.format(mean_squared_error(y_test, sv_pred)))\n",
    "print('The MAE ={}'.format(mean_absolute_error(y_test, sv_pred)))\n",
    "print('The RMSE ={}'.format(np.sqrt(mean_squared_error(y_test, sv_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y_test, sv_pred , scatter = True , label = True , color = 'darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = i)\n",
    "    dtr = DecisionTreeRegressor()\n",
    "    dtr.fit(X_train,y_train)\n",
    "    y_predicted = dtr.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "        \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = 98)\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train , y_train)\n",
    "dtr_pred = dtr.predict(X_test)\n",
    "print('The R2 score={}'.format(r2_score(y_test, dtr_pred)*100))\n",
    "print('The MSE ={}'.format(mean_squared_error(y_test, dtr_pred)))\n",
    "print('The MAE ={}'.format(mean_absolute_error(y_test, dtr_pred)))\n",
    "print('The RMSE ={}'.format(np.sqrt(mean_squared_error(y_test, dtr_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y_test, dtr_pred , scatter = True , label = True , color = 'darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K Nearest Neighbours(KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, train_size = 0.7 , test_size = 0.3, random_state = i)\n",
    "    kn = KNeighborsRegressor()\n",
    "    kn.fit(X_train,y_train)\n",
    "    y_predicted = kn.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "    \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = 29)\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train , y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print('The R2 score={}'.format(r2_score(y_test, knn_pred)*100))\n",
    "print('The MSE ={}'.format(mean_squared_error(y_test, knn_pred)))\n",
    "print('The MAE ={}'.format(mean_absolute_error(y_test, knn_pred)))\n",
    "print('The RMSE ={}'.format(np.sqrt(mean_squared_error(y_test, knn_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y_test, knn_pred , scatter = True , label = True , color = 'darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientBoostRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, train_size = 0.7 , test_size = 0.3, random_state = i)\n",
    "    \n",
    "    gbr = GradientBoostingRegressor()\n",
    "    gbr.fit(X_train,y_train)\n",
    "    y_predicted = gbr.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "    \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = 40)\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train , y_train)\n",
    "gbr_pred = gbr.predict(X_test)\n",
    "print('The R2 score={}'.format(r2_score(y_test, gbr_pred)*100))\n",
    "print('The MSE ={}'.format(mean_squared_error(y_test, gbr_pred)))\n",
    "print('The MAE ={}'.format(mean_absolute_error(y_test, gbr_pred)))\n",
    "print('The RMSE ={}'.format(np.sqrt(mean_squared_error(y_test, gbr_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y_test, gbr_pred , scatter = True , label = True , color = 'darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ExtraTreesRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, train_size = 0.7 , test_size = 0.3, random_state = i)\n",
    "    \n",
    "    etr = ExtraTreesRegressor()\n",
    "    etr.fit(X_train,y_train)\n",
    "    y_predicted = etr.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "    \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = 26)\n",
    "\n",
    "etr = ExtraTreesRegressor()\n",
    "etr.fit(X_train , y_train)\n",
    "etr_pred = etr.predict(X_test)\n",
    "print('The R2 score={}'.format(r2_score(y_test, etr_pred)*100))\n",
    "print('The MSE ={}'.format(mean_squared_error(y_test, etr_pred)))\n",
    "print('The MAE ={}'.format(mean_absolute_error(y_test, etr_pred)))\n",
    "print('The RMSE ={}'.format(np.sqrt(mean_squared_error(y_test, etr_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Regressor hyperparameter Tuning\n",
    "\n",
    "etr = ExtraTreesRegressor()\n",
    "btss = BlockingTimeSeriesSplit(n_splits=5)\n",
    "\n",
    "ETR_params = {\"n_estimators\":[100,200],\n",
    "              \"criterion\":['mse'],\n",
    "              \"max_depth\":[6,8,10,20],\n",
    "             }\n",
    "gsETR = GridSearchCV(etr,param_grid = ETR_params, cv=btss, scoring=\"r2\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsETR.fit(X_train,y_train)\n",
    "\n",
    "ETR_best = gsETR.best_estimator_\n",
    "\n",
    "\n",
    "gsETR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y_test, etr_pred , scatter = True , label = True , color = 'darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoostRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score=0\n",
    "for i in range(2200):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_f, y, train_size = 0.7 , test_size = 0.3, random_state = i)\n",
    "    \n",
    "    abr = AdaBoostRegressor()\n",
    "    abr.fit(X_train,y_train)\n",
    "    y_predicted = abr.predict(X_test)\n",
    "    b_score= r2_score(y_test ,y_predicted )\n",
    "    if b_score>best_score:\n",
    "        best_score=b_score\n",
    "        randomState=i\n",
    "    \n",
    "print('Best Score = {} For Random state = {}'.format(best_score*100,randomState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_f, y, test_size = 0.2, random_state = 26)\n",
    "\n",
    "abr = AdaBoostRegressor()\n",
    "abr.fit(X_train , y_train)\n",
    "abr_pred = abr.predict(X_test)\n",
    "print('The R2 score={}'.format(r2_score(y_test, abr_pred)*100))\n",
    "print('The MSE ={}'.format(mean_squared_error(y_test, abr_pred)))\n",
    "print('The MAE ={}'.format(mean_absolute_error(y_test, abr_pred)))\n",
    "print('The RMSE ={}'.format(np.sqrt(mean_squared_error(y_test, abr_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost Regressor Hyperparameter Tuning \n",
    "\n",
    "\n",
    "abr = AdaBoostRegressor(base_estimator=DecisionTreeRegressor())\n",
    "\n",
    "ABR_params = {\"n_estimators\":[10,50,100,200],\n",
    "              \"learning_rate\":[0.1,0.5,1,2],\n",
    "              \"loss\":[\"linear\",\"square\",\"exponential\"],\n",
    "             }\n",
    "gsAB = GridSearchCV(abr,param_grid = ABR_params, cv=btss, scoring=\"r2\", n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsAB.fit(X_train,y_train)\n",
    "\n",
    "AB_best = gsAB.best_estimator_\n",
    "gsAB.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y_test, abr_pred , scatter = True , label = True , color = 'darkcyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that **Extra Trees Regressor** has the best scores ,\n",
    "so we will use this Algo for our predictions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(etr,'ExtraTreesRegression.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
